{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AUEB M.Sc. in Data Science (part-time)\n",
        "\n",
        "### 2024.04 - 2024.06\n",
        "\n",
        "Exercise 2\n",
        "\n",
        "**Course**: Text Analytics   \n",
        "**Authors**:\n",
        "Anagnos Theodoros (p3352323) -\n",
        "Michalopoulos Ioannis (p3352314) -\n",
        "Kafantaris Panagiotis (p3352328) -  \n",
        "Vigkos Ioannis (p3352326)\n",
        "\n",
        "**Date**: 2024-04-29"
      ],
      "metadata": {
        "id": "mLP61MiTTmmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "installing libraries"
      ],
      "metadata": {
        "id": "DYtDeItVMv1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  tensorflow==2.15.0\n",
        "!pip install -U fasttext"
      ],
      "metadata": {
        "id": "jIi5S4L2tvlZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f13845-306a-4089-cccc-f86f89422978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.15.0 in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.15.0) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjPu6Zu0vGZr",
        "outputId": "718bc451-dfb3-41fa-9686-28786cdb0511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxzU3qM4tvit",
        "outputId": "98933267-d79c-4612-81f4-a0605abd6bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (4.5.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "download dataset"
      ],
      "metadata": {
        "id": "14EW9yFJMy6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-dev.conllu -O en_ewt_dev.conllu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Hw1SjQetvgV",
        "outputId": "d0ebf8be-8b7d-4ca1-fe51-64f7560223f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-29 10:41:25--  https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-dev.conllu\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu [following]\n",
            "--2024-05-29 10:41:25--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1764449 (1.7M) [text/plain]\n",
            "Saving to: ‘en_ewt_dev.conllu’\n",
            "\n",
            "en_ewt_dev.conllu   100%[===================>]   1.68M  8.22MB/s    in 0.2s    \n",
            "\n",
            "2024-05-29 10:41:26 (8.22 MB/s) - ‘en_ewt_dev.conllu’ saved [1764449/1764449]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we tokenize the sentences and build the lists accordingly. We seperate words from tags into different variables for every subset"
      ],
      "metadata": {
        "id": "rNnrF3CVNgsQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Aj6QG2tkc1"
      },
      "outputs": [],
      "source": [
        "from conllu import parse\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def parse_conllu(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "        sentences = parse(data)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def extract_sentences_and_tags(sentences):\n",
        "    all_sentences = []\n",
        "    all_tags = []\n",
        "    for sentence in sentences:\n",
        "        words = []\n",
        "        tags = []\n",
        "        for token in sentence:\n",
        "            if token['form'] and token['upostag']:\n",
        "                words.append(token['form'])\n",
        "                tags.append(token['upostag'])\n",
        "        all_sentences.append(' '.join(words))\n",
        "        all_tags.append(tags)\n",
        "    return all_sentences, all_tags\n",
        "\n",
        "sentences = parse_conllu(\"en_ewt_dev.conllu\")\n",
        "words, tags = extract_words_and_tags(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\", \"lemmatizer\"])\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "nlp.add_pipe('sentencizer')\n",
        "\n",
        "def preprocess(corpus):\n",
        "    corpus_tokenized = []\n",
        "    for doc in tqdm(corpus):\n",
        "        doc = ' '.join(doc)\n",
        "        doc = nlp(doc)\n",
        "        tokens = []\n",
        "        for sent in doc.sents:\n",
        "            for tok in sent:\n",
        "                if '\\n' in tok.text or \"\\t\" in tok.text or \"--\" in tok.text or \"*\" in tok.text or \\\n",
        "                   tok.text.lower() in STOP_WORDS or tok.text in string.punctuation or \\\n",
        "                   all(x in string.punctuation for x in tok.text) or is_number(tok.text):\n",
        "                    continue\n",
        "                if tok.text.strip():\n",
        "                    tokens.append(tok.text.replace('\"', \"'\").strip().lower())\n",
        "        corpus_tokenized.append(tokens)\n",
        "    return corpus_tokenized"
      ],
      "metadata": {
        "id": "54pkOPMj8Npu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sliding Window implementation"
      ],
      "metadata": {
        "id": "4xmxnn3Lf4Ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from nltk.util import ngrams\n",
        "\n",
        "def convert_to_window_dataset(data, window_size, pad_symbol='</s>'):\n",
        "    sentences, pos_tags = data\n",
        "    windowed_sentences = []\n",
        "    windowed_tags = []\n",
        "\n",
        "    half_window = window_size // 2\n",
        "\n",
        "    for sentence, tags in zip(sentences, pos_tags):\n",
        "        padded_sentence = [pad_symbol] * half_window + sentence + [pad_symbol] * half_window\n",
        "        padded_tags = [pad_symbol] * half_window + tags + [pad_symbol] * half_window\n",
        "\n",
        "        for i in range(half_window, len(padded_sentence) - half_window):\n",
        "            windowed_sentence = padded_sentence[i - half_window: i + half_window + 1]\n",
        "            windowed_tag = tags[i - half_window]\n",
        "\n",
        "            if windowed_tag != pad_symbol:\n",
        "                windowed_sentences.append(windowed_sentence)\n",
        "                windowed_tags.append(windowed_tag)\n",
        "\n",
        "    return windowed_sentences, windowed_tags\n",
        "\n",
        "window_size = 3\n",
        "\n",
        "# data = (\n",
        "#     [['the', 'cat', 'sat', 'on', 'the', 'mat'], ['another', 'sentence']],\n",
        "#     [['DET', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN'], ['DET', 'NOUN']]\n",
        "# )\n",
        "\n",
        "# windowed_sentences, windowed_tags = convert_to_window_dataset(data, window_size)\n",
        "# print(\"Windowed Sentences:\", windowed_sentences)\n",
        "# print(\"Windowed Tags:\", windowed_tags)"
      ],
      "metadata": {
        "id": "BC-Si31jecIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parsed_sentences = parse_conllu(file_path)\n",
        "raw_sentences, tags = extract_sentences_and_tags(parsed_sentences)\n",
        "\n",
        "# Split the data into train, dev, and test sets\n",
        "train_sentences, temp_sentences, train_tags, temp_tags = train_test_split(raw_sentences, tags, test_size=0.4, random_state=42)\n",
        "dev_sentences, test_sentences, dev_tags, test_tags = train_test_split(temp_sentences, temp_tags, test_size=0.5, random_state=42)\n",
        "\n",
        "# Combine all sentences for creating vocabulary\n",
        "all_sentences = train_sentences + dev_sentences + test_sentences\n",
        "\n",
        "# Preprocess the combined sentences to create tokens\n",
        "all_sentences_tokenized = preprocess(all_sentences)\n",
        "\n",
        "# Flatten the tokenized sentences\n",
        "flat_all_sentences = [word for sublist in all_sentences_tokenized for word in sublist]\n",
        "\n",
        "# Create vocabulary for words from the complete dataset to ensure comprehensive coverage\n",
        "vocab = {word: idx for idx, word in enumerate(set(flat_all_sentences))}\n",
        "\n",
        "# Create vocabulary for tags\n",
        "flat_all_tags = [tag for sublist in tags for tag in sublist]\n",
        "vocab_tags = {tag: idx for idx, tag in enumerate(set(flat_all_tags))}\n"
      ],
      "metadata": {
        "id": "vHckU_ksZTNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess individual sets\n",
        "train_words_tokenized = preprocess(train_words)\n",
        "dev_words_tokenized = preprocess(dev_words)\n",
        "test_words_tokenized = preprocess(test_words)"
      ],
      "metadata": {
        "id": "Z0Jfw35it8lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Build the vocabulary\n",
        "# flat_train_words = [word for sublist in train_words for word in sublist]\n",
        "# vocab = {word: idx for idx, word in enumerate(set(flat_train_words))}\n",
        "# vocab_tags = {tag: idx for idx, tag in enumerate(set(train_tags))}\n",
        "\n",
        "all_words = train_words + dev_words + test_words\n",
        "all_words_tokenized = preprocess(all_words)\n",
        "flat_all_words = [word for sublist in all_words_tokenized for word in sublist]\n",
        "# Create vocabulary from the complete dataset\n",
        "vocab = {word: idx for idx, word in enumerate(set(flat_all_words))}\n",
        "\n",
        "# Create vocabulary for tags\n",
        "flat_all_tags = [tag for sublist in all_tags for tag in sublist]\n",
        "vocab_tags = {tag: idx for idx, tag in enumerate(set(flat_all_tags))}"
      ],
      "metadata": {
        "id": "vN0QshT2h1mD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "73ffb934-b1ba-4228-9bce-339ea4460fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b0b0e8688588>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mall_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdev_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mall_words_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mflat_all_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msublist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_words_tokenized\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msublist\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create vocabulary from the complete dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab), len(vocab_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gziiffNnrqXO",
        "outputId": "e61534a3-c79b-413a-cab1-5b58b0592673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3627, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the W2V embendings"
      ],
      "metadata": {
        "id": "LmYTUC7UNxCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d cc.en.300.bin.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzR83oIw4uOl",
        "outputId": "cb7d1b71-1f27-4655-f9ae-39b61b2d1a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-29 11:12:09--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 54.192.18.54, 54.192.18.51, 54.192.18.50, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|54.192.18.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G   124MB/s    in 51s     \n",
            "\n",
            "2024-05-29 11:13:01 (84.7 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gensim.downloader as api\n",
        "\n",
        "# wv = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "sHtyvecwuOlT",
        "outputId": "4f4ce104-79b9-4790-c680-0fa1fa52fd12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map each word with its embending matrix. We do the same for every subset. We initialize with zeros, each words that isnt included in the vocabulary (wv)"
      ],
      "metadata": {
        "id": "rQ6XM470N2HW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to map words to embeddings\n",
        "def map_vocab_to_embeddings(vocab, wv, embedding_dim=300):\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
        "    for word, idx in vocab.items():\n",
        "        if word in wv:\n",
        "            embedding_matrix[idx] = wv[word]\n",
        "        else:\n",
        "            # Handle out-of-vocabulary (OOV) words by initializing with zeros\n",
        "            embedding_matrix[idx] = np.zeros(embedding_dim)\n",
        "    return embedding_matrix\n",
        "\n",
        "# Map vocab to embeddings\n",
        "embeddings = map_vocab_to_embeddings(vocab, wv)\n"
      ],
      "metadata": {
        "id": "ecEz8VUTpWpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "id": "nvXz39kmoS1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "090ca3b9-7186-463f-fcba-a31f29681607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3627, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we build the one-hot encoding for each subset in order to use it as a feature in our model later."
      ],
      "metadata": {
        "id": "wLLpcOIlOM-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Encode POS tags\n",
        "label_encoder = LabelEncoder()\n",
        "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "train_tags_encoded = onehot_encoder.fit_transform(label_encoder.fit_transform(train_tags).reshape(-1, 1))\n",
        "dev_tags_encoded = onehot_encoder.transform(label_encoder.transform(dev_tags).reshape(-1, 1))\n",
        "test_tags_encoded = onehot_encoder.transform(label_encoder.transform(test_tags).reshape(-1, 1))\n"
      ],
      "metadata": {
        "id": "l47yDSUW3NUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert words to indices\n",
        "def words_to_indices(words, vocab):\n",
        "    return np.array([[vocab.get(word, 0) for word in window] for window in words])\n",
        "\n",
        "train_words_idx = words_to_indices(train_words, vocab)\n",
        "dev_words_idx = words_to_indices(dev_words, vocab)\n",
        "test_words_idx = words_to_indices(test_words, vocab)"
      ],
      "metadata": {
        "id": "1bUzEoau7HTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define our MLP model. We start from a random architecture and later we do Hyperparameter tuning. We set EarlyStopping to patience=3. Which means that after 3 epochs that validation loss will be worse the process will stop and will save the best weights (restore_best_weights=True)"
      ],
      "metadata": {
        "id": "lrqsjAdSOmu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input, Embedding, Flatten, Bidirectional, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, auc, f1_score, recall_score, precision_score\n",
        "\n",
        "model = Sequential([\n",
        "        Input(shape=(3,), dtype='int32', name='Input_Layer'),\n",
        "        Embedding(len(vocab), 300, weights=[embeddings], input_length=3, trainable=True),\n",
        "        Bidirectional(LSTM(256, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(len(vocab_tags), activation='softmax')  # Output layer multi-class classification\n",
        "    ])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=[CategoricalAccuracy()])\n",
        "\n",
        "\n",
        "# Metrics callback\n",
        "class Metrics(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, valid_data):\n",
        "        super(Metrics, self).__init__()\n",
        "        self.validation_data = valid_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_predict = np.argmax(self.model.predict(self.validation_data[0]), -1)\n",
        "        val_targ = np.argmax(self.validation_data[1], -1)\n",
        "\n",
        "        _val_f1 = f1_score(val_targ, val_predict, average=\"weighted\")\n",
        "        _val_recall = recall_score(val_targ, val_predict, average=\"weighted\")\n",
        "        _val_precision = precision_score(val_targ, val_predict, average=\"weighted\")\n",
        "\n",
        "        logs['val_f1'] = _val_f1\n",
        "        logs['val_recall'] = _val_recall\n",
        "        logs['val_precision'] = _val_precision\n",
        "        print(f\" — val_f1: {_val_f1} — val_precision: {_val_precision} — val_recall: {_val_recall}\")\n",
        "        return\n",
        "\n",
        "early_stopping = EarlyStopping(patience=10, verbose=2, restore_best_weights=True, monitor='val_loss', mode='min')\n",
        "\n",
        "history = model.fit(\n",
        "    train_words_idx, train_tags_encoded,\n",
        "    validation_data=(dev_words_idx, dev_tags_encoded),\n",
        "    batch_size=256, epochs=100, shuffle=True,\n",
        "    callbacks=[Metrics(valid_data=(dev_words_idx, dev_tags_encoded)), early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "kefyETsCHaS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b3a21a-b533-46a6-b188-1c011311099e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 3, 300)            1088100   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 512)               1140736   \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               65664     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 18)                2322      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2296822 (8.76 MB)\n",
            "Trainable params: 2296822 (8.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "160/160 [==============================] - 1s 4ms/step\n",
            " — val_f1: 0.3369201463516517 — val_precision: 0.34571468354696044 — val_recall: 0.3706389651117209\n",
            "60/60 [==============================] - 19s 140ms/step - loss: 2.4428 - categorical_accuracy: 0.2411 - val_loss: 2.0121 - val_categorical_accuracy: 0.3706 - val_f1: 0.3369 - val_recall: 0.3706 - val_precision: 0.3457\n",
            "Epoch 2/100\n",
            " 1/60 [..............................] - ETA: 1s - loss: 2.0020 - categorical_accuracy: 0.3984"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 3ms/step\n",
            " — val_f1: 0.520409439681463 — val_precision: 0.5533561630377771 — val_recall: 0.5560564484515876\n",
            "60/60 [==============================] - 6s 99ms/step - loss: 1.8243 - categorical_accuracy: 0.4256 - val_loss: 1.4618 - val_categorical_accuracy: 0.5561 - val_f1: 0.5204 - val_recall: 0.5561 - val_precision: 0.5534\n",
            "Epoch 3/100\n",
            " 7/60 [==>...........................] - ETA: 1s - loss: 1.3731 - categorical_accuracy: 0.5787"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 5ms/step\n",
            " — val_f1: 0.7606067796227609 — val_precision: 0.8108270535292319 — val_recall: 0.7628381027048217\n",
            "60/60 [==============================] - 4s 62ms/step - loss: 1.0437 - categorical_accuracy: 0.6867 - val_loss: 0.8931 - val_categorical_accuracy: 0.7628 - val_f1: 0.7606 - val_recall: 0.7628 - val_precision: 0.8108\n",
            "Epoch 4/100\n",
            " 6/60 [==>...........................] - ETA: 1s - loss: 0.6864 - categorical_accuracy: 0.7917"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 8ms/step\n",
            " — val_f1: 0.7934596038291534 — val_precision: 0.8406393716412542 — val_recall: 0.7885143081144649\n",
            "60/60 [==============================] - 6s 94ms/step - loss: 0.6227 - categorical_accuracy: 0.8177 - val_loss: 0.7919 - val_categorical_accuracy: 0.7885 - val_f1: 0.7935 - val_recall: 0.7885 - val_precision: 0.8406\n",
            "Epoch 5/100\n",
            " 2/60 [>.............................] - ETA: 4s - loss: 0.5095 - categorical_accuracy: 0.8711"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 4ms/step\n",
            " — val_f1: 0.8037838408101826 — val_precision: 0.8506480645686444 — val_recall: 0.7951783614268915\n",
            "60/60 [==============================] - 5s 85ms/step - loss: 0.4578 - categorical_accuracy: 0.8695 - val_loss: 0.9167 - val_categorical_accuracy: 0.7952 - val_f1: 0.8038 - val_recall: 0.7952 - val_precision: 0.8506\n",
            "Epoch 6/100\n",
            " 1/60 [..............................] - ETA: 10s - loss: 0.4331 - categorical_accuracy: 0.8789"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 3ms/step\n",
            " — val_f1: 0.8072601379767056 — val_precision: 0.846796040243656 — val_recall: 0.800078400627205\n",
            "60/60 [==============================] - 3s 41ms/step - loss: 0.3812 - categorical_accuracy: 0.8881 - val_loss: 0.8920 - val_categorical_accuracy: 0.8001 - val_f1: 0.8073 - val_recall: 0.8001 - val_precision: 0.8468\n",
            "Epoch 7/100\n",
            " 5/60 [=>............................] - ETA: 1s - loss: 0.2780 - categorical_accuracy: 0.9125"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 3ms/step\n",
            " — val_f1: 0.809885162617824 — val_precision: 0.8542421486761392 — val_recall: 0.8012544100352803\n",
            "60/60 [==============================] - 3s 43ms/step - loss: 0.3177 - categorical_accuracy: 0.9057 - val_loss: 1.0958 - val_categorical_accuracy: 0.8013 - val_f1: 0.8099 - val_recall: 0.8013 - val_precision: 0.8542\n",
            "Epoch 8/100\n",
            " 6/60 [==>...........................] - ETA: 1s - loss: 0.2733 - categorical_accuracy: 0.9180"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 5ms/step\n",
            " — val_f1: 0.8080520559097703 — val_precision: 0.8537178535149493 — val_recall: 0.8008624068992551\n",
            "60/60 [==============================] - 4s 61ms/step - loss: 0.2743 - categorical_accuracy: 0.9171 - val_loss: 1.1752 - val_categorical_accuracy: 0.8009 - val_f1: 0.8081 - val_recall: 0.8009 - val_precision: 0.8537\n",
            "Epoch 9/100\n",
            " 3/60 [>.............................] - ETA: 3s - loss: 0.2755 - categorical_accuracy: 0.9310"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 3ms/step\n",
            " — val_f1: 0.807708359020003 — val_precision: 0.8508569879913206 — val_recall: 0.7989023912191298\n",
            "60/60 [==============================] - 3s 44ms/step - loss: 0.2509 - categorical_accuracy: 0.9234 - val_loss: 1.1530 - val_categorical_accuracy: 0.7989 - val_f1: 0.8077 - val_recall: 0.7989 - val_precision: 0.8509\n",
            "Epoch 10/100\n",
            " 7/60 [==>...........................] - ETA: 1s - loss: 0.2114 - categorical_accuracy: 0.9364"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 3ms/step\n",
            " — val_f1: 0.8111193943078374 — val_precision: 0.8557713882549575 — val_recall: 0.8030184241473932\n",
            "60/60 [==============================] - 3s 50ms/step - loss: 0.2212 - categorical_accuracy: 0.9316 - val_loss: 1.2543 - val_categorical_accuracy: 0.8030 - val_f1: 0.8111 - val_recall: 0.8030 - val_precision: 0.8558\n",
            "Epoch 11/100\n",
            " 6/60 [==>...........................] - ETA: 1s - loss: 0.1825 - categorical_accuracy: 0.9382"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 3ms/step\n",
            " — val_f1: 0.8102085481522047 — val_precision: 0.8546330959222207 — val_recall: 0.8018424147393179\n",
            "60/60 [==============================] - 3s 44ms/step - loss: 0.2049 - categorical_accuracy: 0.9343 - val_loss: 1.3382 - val_categorical_accuracy: 0.8018 - val_f1: 0.8102 - val_recall: 0.8018 - val_precision: 0.8546\n",
            "Epoch 12/100\n",
            " 6/60 [==>...........................] - ETA: 1s - loss: 0.1763 - categorical_accuracy: 0.9447"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 [==============================] - 1s 4ms/step\n",
            " — val_f1: 0.8110326620615863 — val_precision: 0.8551489636426974 — val_recall: 0.8020384163073304\n",
            "60/60 [==============================] - 3s 51ms/step - loss: 0.1836 - categorical_accuracy: 0.9414 - val_loss: 1.3626 - val_categorical_accuracy: 0.8020 - val_f1: 0.8110 - val_recall: 0.8020 - val_precision: 0.8551\n",
            "Epoch 13/100\n",
            "160/160 [==============================] - 1s 4ms/step\n",
            " — val_f1: 0.8116437455080099 — val_precision: 0.8551809844949367 — val_recall: 0.8016464131713054\n",
            "60/60 [==============================] - 3s 55ms/step - loss: 0.1706 - categorical_accuracy: 0.9462 - val_loss: 1.4295 - val_categorical_accuracy: 0.8016 - val_f1: 0.8116 - val_recall: 0.8016 - val_precision: 0.8552\n",
            "Epoch 14/100\n",
            "160/160 [==============================] - 1s 3ms/step\n",
            " — val_f1: 0.8129033779942265 — val_precision: 0.8598117096371681 — val_recall: 0.8034104272834183\n",
            "Restoring model weights from the end of the best epoch: 4.\n",
            "60/60 [==============================] - 2s 40ms/step - loss: 0.1509 - categorical_accuracy: 0.9523 - val_loss: 1.6579 - val_categorical_accuracy: 0.8034 - val_f1: 0.8129 - val_recall: 0.8034 - val_precision: 0.8598\n",
            "Epoch 14: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyper-parameter tuning"
      ],
      "metadata": {
        "id": "fWTCT-p7rSjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U keras-tuner"
      ],
      "metadata": {
        "id": "pIlUIj_HjI7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.layers import GRU\n",
        "from keras_tuner import RandomSearch, Objective\n",
        "\n",
        "def build_tunable_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(vocab), 300, weights=[embeddings], input_length=3, trainable=True))\n",
        "    for i in range(hp.Int('num_layers', 1, 5)):\n",
        "        if hp.Choice('rnn_type', ['LSTM', 'GRU']) == 'LSTM':\n",
        "            model.add(Bidirectional(LSTM(units=hp.Int('units_' + str(i), min_value=128, max_value=512, step=64),\n",
        "                                         return_sequences=True if i < hp.Int('num_layers', 1, 5) - 1 else False,\n",
        "                                         dropout=hp.Choice('dropout_' + str(i), values=[0.3, 0.5, 0.7]),\n",
        "                                         recurrent_dropout=hp.Choice('recurrent_dropout_' + str(i), values=[0.3, 0.5, 0.7]),\n",
        "                                         kernel_regularizer=l2(0.01))))\n",
        "        else:\n",
        "            model.add(Bidirectional(GRU(units=hp.Int('units_' + str(i), min_value=128, max_value=512, step=64),\n",
        "                                        return_sequences=True if i < hp.Int('num_layers', 1, 5) - 1 else False,\n",
        "                                        dropout=hp.Choice('dropout_' + str(i), values=[0.3, 0.5, 0.7]),\n",
        "                                        recurrent_dropout=hp.Choice('recurrent_dropout_' + str(i), values=[0.3, 0.5, 0.7]),\n",
        "                                        kernel_regularizer=l2(0.01))))\n",
        "    model.add(Dense(len(vocab_tags), activation='softmax'))\n",
        "    model.compile(optimizer=Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4])),\n",
        "                  loss='categorical_crossentropy', metrics=[CategoricalAccuracy()])\n",
        "    return model\n",
        "\n",
        "# Instantiate the tuner\n",
        "tuner = RandomSearch(build_tunable_model, objective=Objective(\"val_categorical_accuracy\", direction=\"max\"),\n",
        "                     max_trials=10, executions_per_trial=1, directory='KT_dir', project_name='KT_tuning')\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=2)\n",
        "\n",
        "# Search for the best hyperparameters\n",
        "tuner.search(train_words_idx, train_tags_encoded, epochs=80, validation_data=(dev_words_idx, dev_tags_encoded),\n",
        "             callbacks=[early_stopping])\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "loss, accuracy = best_model.evaluate(test_words_idx, test_tags_encoded)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "best_model.save(\"biRNN_model.keras\")"
      ],
      "metadata": {
        "id": "ij3Jbcx5dGMi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "14ddfeb0-7935-4cd7-da0c-2664a22fc7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8 Complete [00h 05m 48s]\n",
            "val_categorical_accuracy: 0.7842022776603699\n",
            "\n",
            "Best val_categorical_accuracy So Far: 0.803214430809021\n",
            "Total elapsed time: 01h 03m 40s\n",
            "\n",
            "Search: Running Trial #9\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "4                 |1                 |num_layers\n",
            "LSTM              |GRU               |rnn_type\n",
            "192               |448               |units_0\n",
            "0.3               |0.5               |dropout_0\n",
            "0.3               |0.7               |recurrent_dropout_0\n",
            "0.001             |0.001             |learning_rate\n",
            "512               |128               |units_1\n",
            "0.7               |0.5               |dropout_1\n",
            "0.5               |0.7               |recurrent_dropout_1\n",
            "384               |512               |units_2\n",
            "0.5               |0.5               |dropout_2\n",
            "0.5               |0.3               |recurrent_dropout_2\n",
            "448               |None              |units_3\n",
            "0.7               |None              |dropout_3\n",
            "0.5               |None              |recurrent_dropout_3\n",
            "192               |None              |units_4\n",
            "0.7               |None              |dropout_4\n",
            "0.5               |None              |recurrent_dropout_4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "479/479 [==============================] - 85s 124ms/step - loss: 4.6278 - categorical_accuracy: 0.1611 - val_loss: 2.5468 - val_categorical_accuracy: 0.1613\n",
            "Epoch 2/80\n",
            " 26/479 [>.............................] - ETA: 41s - loss: 2.5384 - categorical_accuracy: 0.1839"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-238b66230bc3>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Search for the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m tuner.search(train_words_idx, train_tags_encoded, epochs=80, validation_data=(dev_words_idx, dev_tags_encoded),\n\u001b[0m\u001b[1;32m     36\u001b[0m              callbacks=[early_stopping])\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_and_update_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         if self.oracle.get_trial(trial.trial_id).metrics.exists(\n\u001b[1;32m    241\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mobj_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_and_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mhistories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# Save the build config for model loading later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras_tuner/src/engine/hypermodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \"\"\"\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Curves')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Save accuracy plots\n",
        "plt.figure()\n",
        "plt.plot(history.history['categorical_accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_categorical_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy Curves')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "# plt.savefig('accuracy_plot.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CKwbMePfd6LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best hyperparameters: {best_hyperparameters.values}\")"
      ],
      "metadata": {
        "id": "Tefw5jdJ3Evh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline: Most frequent tag\n",
        "most_frequent_tag = max(set(train_tags), key=train_tags.count)\n",
        "baseline_predictions = [most_frequent_tag] * len(test_tags)\n",
        "baseline_predictions_encoded = label_encoder.transform(baseline_predictions)\n",
        "\n",
        "# Calculate classification report\n",
        "def evaluate_model(predictions, true_tags, labels):\n",
        "    print(classification_report(true_tags, predictions, target_names=labels))\n",
        "\n",
        "# Evaluate on test set\n",
        "test_predictions = np.argmax(model.predict(test_words_idx), axis=-1)\n",
        "test_true = np.argmax(test_tags_encoded, axis=-1)\n",
        "\n",
        "print(\"Test Set Classification Report:\")\n",
        "evaluate_model(test_predictions, test_true, label_encoder.classes_)\n",
        "\n",
        "# Evaluate on development set\n",
        "dev_predictions = np.argmax(model.predict(dev_words_idx), axis=-1)\n",
        "dev_true = np.argmax(dev_tags_encoded, axis=-1)\n",
        "\n",
        "print(\"Development Set Classification Report:\")\n",
        "evaluate_model(dev_predictions, dev_true, label_encoder.classes_)\n",
        "\n",
        "# Evaluate on training set\n",
        "train_predictions = np.argmax(model.predict(train_words_idx), axis=-1)\n",
        "train_true = np.argmax(train_tags_encoded, axis=-1)\n",
        "\n",
        "print(\"Training Set Classification Report:\")\n",
        "evaluate_model(train_predictions, train_true, label_encoder.classes_)\n",
        "\n",
        "# Evaluate baseline\n",
        "baseline_true = test_true\n",
        "print(\"Baseline Classification Report:\")\n",
        "evaluate_model(baseline_predictions_encoded, baseline_true, label_encoder.classes_)\n",
        "\n",
        "# Describe the methods and datasets\n",
        "num_train_sentences = len(train_words) // 3\n",
        "num_dev_sentences = len(dev_words) // 3\n",
        "num_test_sentences = len(test_words) // 3\n",
        "num_train_words = len(train_words)\n",
        "num_dev_words = len(dev_words)\n",
        "num_test_words = len(test_words)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Number of training sentences: {num_train_sentences}\")\n",
        "print(f\"Number of development sentences: {num_dev_sentences}\")\n",
        "print(f\"Number of test sentences: {num_test_sentences}\")\n",
        "print(f\"Number of training words: {num_train_words}\")\n",
        "print(f\"Number of development words: {num_dev_words}\")\n",
        "print(f\"Number of test words: {num_test_words}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")"
      ],
      "metadata": {
        "id": "sxz-jBcsfrNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "example"
      ],
      "metadata": {
        "id": "A8XbKJL3SuRo"
      }
    }
  ]
}