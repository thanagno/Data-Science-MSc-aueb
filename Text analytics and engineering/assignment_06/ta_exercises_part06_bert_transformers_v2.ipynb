{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ddb09145-bd5c-4f8a-98be-a9b8d9a21d72",
      "metadata": {
        "id": "ddb09145-bd5c-4f8a-98be-a9b8d9a21d72"
      },
      "source": [
        "# AUEB M.Sc. in Data Science (part-time)\n",
        "\n",
        "### 2024.04 - 2024.06\n",
        "\n",
        "## PART 06\n",
        "### EXERCISE 01: Fine-Tuning BERT for Text Classification and Named Entity Recognition using HuggingFace Transformers\n",
        "\n",
        "\n",
        "**Course**: Text Analytics   \n",
        "**Authors**:\n",
        "Anagnos Theodoros (p3352323) -\n",
        "Michalopoulos Ioannis (p3352314) -\n",
        "Kafantaris Panagiotis (p3352328) -  \n",
        "Vigkos Ioannis (p3352326)\n",
        "\n",
        "**Date**: 2024-06-11"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e6d427-d03e-4df4-ad7e-fcc03f37ae04",
      "metadata": {
        "id": "29e6d427-d03e-4df4-ad7e-fcc03f37ae04"
      },
      "source": [
        "### Setings and install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84255a7e-b83f-4e2f-8c35-d391e76d6112",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84255a7e-b83f-4e2f-8c35-d391e76d6112",
        "outputId": "1559b25a-2406-4623-ea15-5c25478c31ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.30)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc8fa64-9f67-4881-b38f-9f0a26d4efb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebc8fa64-9f67-4881-b38f-9f0a26d4efb0",
        "outputId": "2e7dc96e-05ac-4283-cb87-82c6fb035f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f08ef36-0d43-41ca-887c-33b37ada93b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f08ef36-0d43-41ca-887c-33b37ada93b4",
        "outputId": "3ba811c6-54f5-44f5-fa30-0201f3722097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d5d566-a309-4161-a9ba-e210df3d32ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5d5d566-a309-4161-a9ba-e210df3d32ec",
        "outputId": "1c67b8ad-5f6b-4ca1-a88f-ce02c937d5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d701c31-f444-46b6-9fff-28db67948c0f",
      "metadata": {
        "id": "3d701c31-f444-46b6-9fff-28db67948c0f"
      },
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63ebdfa-3a71-422b-ab22-0553cdf219bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d63ebdfa-3a71-422b-ab22-0553cdf219bb",
        "outputId": "bf4d8ed0-0454-49b4-8557-99af34acf550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# NLTK Downloads and Imports\n",
        "# Downloading necessary NLTK resources for tokenization, stopwords, lemmatization, and POS tagging\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bff47ab-59d0-4ca9-bb00-cf067db6516d",
      "metadata": {
        "id": "1bff47ab-59d0-4ca9-bb00-cf067db6516d"
      },
      "outputs": [],
      "source": [
        "# General Purpose Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import re\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Text Preprocessing Libraries\n",
        "import contractions\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, precision_recall_fscore_support, average_precision_score, precision_recall_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Transformers Libraries for BERT\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, TrainerCallback\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization, Embedding, Dropout, Bidirectional, LSTM, Conv1D, GlobalMaxPooling1D, Dense, Input, concatenate\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Optimization Libraries\n",
        "import optuna\n",
        "\n",
        "# Visualization Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PyTorch Libraries\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "650eb563-81a0-464c-9ad0-cf545cec6460",
      "metadata": {
        "id": "650eb563-81a0-464c-9ad0-cf545cec6460"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397bd665-fa07-4364-995a-da59405afc72",
      "metadata": {
        "id": "397bd665-fa07-4364-995a-da59405afc72"
      },
      "outputs": [],
      "source": [
        "# Defining a function for converting chat words to their full forms\n",
        "def convert_chat_words(text):\n",
        "    # Splitting the text into individual words\n",
        "    words = text.split()\n",
        "    converted_words = []\n",
        "\n",
        "    # Iterating over each word in the text\n",
        "    for word in words:\n",
        "        # Checking if the word is in the chat_words_dict\n",
        "        if word.lower() in chat_words_dict:\n",
        "            # Converting the chat word to its full form\n",
        "            converted_words.append(chat_words_dict[word.lower()])\n",
        "        else:\n",
        "            # Keeping the word as it is if it's not a chat word\n",
        "            converted_words.append(word)\n",
        "\n",
        "    # Joining the converted words back into a single string\n",
        "    converted_text = \" \".join(converted_words)\n",
        "    return converted_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa38d2e1-f58f-4e21-b829-dff423efa2ac",
      "metadata": {
        "id": "fa38d2e1-f58f-4e21-b829-dff423efa2ac"
      },
      "outputs": [],
      "source": [
        "# Defining a function for cleaning text by removing punctuation, numbers, extra spaces, and repetitions of punctuation\n",
        "def clean_text(text):\n",
        "    # Removing punctuation from the text\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Removing numbers from the text\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Removing extra spaces from the text\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    # Replacing repetitions of punctuation in the text\n",
        "    text = re.sub(r'(\\W)\\1+', r'\\1', text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24d48116-75a8-4850-847d-8faf4d555e80",
      "metadata": {
        "id": "24d48116-75a8-4850-847d-8faf4d555e80"
      },
      "outputs": [],
      "source": [
        "# Defining a function for removing special characters from the text\n",
        "def remove_special_characters(text):\n",
        "    # Removing special characters from the text\n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e57bac52-5adb-4350-896e-600bb2151cd0",
      "metadata": {
        "id": "e57bac52-5adb-4350-896e-600bb2151cd0"
      },
      "outputs": [],
      "source": [
        "# Defining a function for performing lemmatization on text\n",
        "def lemmatize_text(text):\n",
        "    # Getting the POS tags for the words\n",
        "    pos_tags = nltk.pos_tag(text)\n",
        "\n",
        "    # Performing lemmatization\n",
        "    lemmatized_words = []\n",
        "    for word, tag in pos_tags:\n",
        "        # Mapping the POS tag to the WordNet POS tag\n",
        "        pos = wordnet_map.get(tag[0].upper(), wordnet.NOUN)\n",
        "        # Lemmatizing the word with the appropriate POS tag\n",
        "        lemmatized_word = lemmatizer.lemmatize(word, pos=pos)\n",
        "        # Adding the lemmatized word to the list\n",
        "        lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d1933e-5694-45e5-bd00-334dcc973001",
      "metadata": {
        "id": "51d1933e-5694-45e5-bd00-334dcc973001"
      },
      "outputs": [],
      "source": [
        "def model_init():\n",
        "    # Loading the BERT model for sequence classification from the pretrained 'bert-base-uncased' model\n",
        "    # Setting the number of labels according to the length of the label mapping\n",
        "    return BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ddc928-2d60-408e-8d79-e7156cf2e2c2",
      "metadata": {
        "id": "54ddc928-2d60-408e-8d79-e7156cf2e2c2"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    # Calculating predictions by taking the argmax of the last dimension\n",
        "    preds = p.predictions.argmax(-1)\n",
        "\n",
        "    # Loading true labels\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Calculating precision using the weighted average method\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "\n",
        "    # Calculating recall using the weighted average method\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    # Calculating F1 score using the weighted average method\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "\n",
        "    # Calculating accuracy\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    # Returning a dictionary of computed metrics\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89527719-35cd-4aa7-8894-4565f712fc92",
      "metadata": {
        "id": "89527719-35cd-4aa7-8894-4565f712fc92"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Defining hyperparameters to tune\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 0.01, 0.1, log=True)\n",
        "    warmup_steps = trial.suggest_int('warmup_steps', 0, 500)\n",
        "    num_train_epochs = trial.suggest_int('num_train_epochs', 3, 5)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
        "\n",
        "    # Defining training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',  # Setting output directory\n",
        "        learning_rate=learning_rate,  # Setting learning rate\n",
        "        per_device_train_batch_size=batch_size,  # Setting batch size for training\n",
        "        per_device_eval_batch_size=batch_size,  # Setting batch size for evaluation\n",
        "        num_train_epochs=num_train_epochs,  # Setting number of training epochs\n",
        "        weight_decay=weight_decay,  # Setting weight decay\n",
        "        warmup_steps=warmup_steps,  # Setting warmup steps\n",
        "        evaluation_strategy=\"steps\",  # Setting evaluation strategy to steps\n",
        "        eval_steps=100,  # Setting evaluation steps\n",
        "        save_steps=100,  # Setting save steps\n",
        "        load_best_model_at_end=True,  # Enabling loading best model at end\n",
        "        metric_for_best_model=\"accuracy\",  # Setting metric for best model\n",
        "        logging_dir='./logs',  # Setting logging directory\n",
        "        logging_steps=10  # Setting logging steps\n",
        "    )\n",
        "\n",
        "    # Initializing the trainer\n",
        "    trainer = Trainer(\n",
        "        model_init=model_init,  # Initializing model\n",
        "        args=training_args,  # Setting training arguments\n",
        "        train_dataset=train_dataset,  # Setting training dataset\n",
        "        eval_dataset=val_dataset,  # Setting validation dataset\n",
        "        compute_metrics=compute_metrics,  # Setting compute metrics function\n",
        "    )\n",
        "\n",
        "    # Training the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluating the model on the validation dataset\n",
        "    eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
        "\n",
        "    # Returning the evaluation accuracy\n",
        "    return eval_results['eval_accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aabcb36-2400-47ef-ab8f-ac50ffa750ed",
      "metadata": {
        "id": "5aabcb36-2400-47ef-ab8f-ac50ffa750ed"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(y_test, y_pred, y_proba, classes):\n",
        "    # Calculating macro-averaged precision, recall, and F1 score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
        "    print(f'Macro-averaged Precision: {precision:.4f}')\n",
        "    print(f'Macro-averaged Recall: {recall:.4f}')\n",
        "    print(f'Macro-averaged F1 Score: {f1:.4f}')\n",
        "\n",
        "    # Binarizing the labels\n",
        "    y_test_binarized = label_binarize(y_test, classes=classes)\n",
        "    n_classes = y_test_binarized.shape[1]\n",
        "\n",
        "    # Initializing list to store PR-AUC scores for each class\n",
        "    pr_auc_scores = []\n",
        "\n",
        "    # Computing PR-AUC for each class\n",
        "    for i in range(n_classes):\n",
        "        pr_auc = average_precision_score(y_test_binarized[:, i], y_proba[:, i])\n",
        "        pr_auc_scores.append(pr_auc)\n",
        "\n",
        "    # Calculating macro-averaged PR-AUC\n",
        "    macro_pr_auc = np.mean(pr_auc_scores)\n",
        "    print(f'Macro-averaged PR-AUC: {macro_pr_auc:.4f}')\n",
        "\n",
        "    # Printing detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to compute metrics\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "wO3dC_wa0h-V"
      },
      "id": "wO3dC_wa0h-V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot Precision-Recall AUC for each class\n",
        "def plot_precision_recall_auc(y_test, y_proba, class_names, model_name):\n",
        "    # Calculating the number of classes and setting up the subplot grid\n",
        "    num_classes = len(class_names)\n",
        "    num_cols = 3\n",
        "    num_rows = (num_classes + num_cols - 1) // num_cols\n",
        "\n",
        "    # Creating a subplot grid\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Calculating Precision-Recall AUC for each class and plotting the Precision-Recall curve\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        precision, recall, _ = precision_recall_curve(y_test == i, y_proba[:, i])\n",
        "        pr_auc = auc(recall, precision)\n",
        "        print(f'Class {class_name}: Precision-Recall AUC = {pr_auc:.4f}')\n",
        "\n",
        "        # Plotting the Precision-Recall curve in the corresponding subplot\n",
        "        axes[i].plot(recall, precision, label=f'AUC={pr_auc:.4f}')\n",
        "        axes[i].set_title(f'Class {class_name}')\n",
        "        axes[i].set_xlabel('Recall')\n",
        "        axes[i].set_ylabel('Precision')\n",
        "        axes[i].set_ylim([0.0, 1.05])\n",
        "        axes[i].set_xlim([0.0, 1.0])\n",
        "        axes[i].legend(loc=\"lower left\")\n",
        "\n",
        "    # Removing any empty subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        fig.delaxes(axes[j])\n",
        "\n",
        "    # Adjusting layout and setting the main title\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(f'Precision-Recall Curves for {model_name}', y=1.02)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MiSoQQzc0jx_"
      },
      "id": "MiSoQQzc0jx_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "97f2fdf9-1101-47b3-9782-06f15528d8c3",
      "metadata": {
        "id": "97f2fdf9-1101-47b3-9782-06f15528d8c3"
      },
      "source": [
        "## Data Preprocessing and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c8571da-0a27-4f10-931c-a40c21c5eaeb",
      "metadata": {
        "id": "0c8571da-0a27-4f10-931c-a40c21c5eaeb"
      },
      "outputs": [],
      "source": [
        "# Setting display options to show all columns and rows\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "396c5457-0ed4-437f-99c3-f47fe687b939",
      "metadata": {
        "id": "396c5457-0ed4-437f-99c3-f47fe687b939"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ffdd04c-6ac8-406e-b49a-151e50a0a099",
      "metadata": {
        "id": "7ffdd04c-6ac8-406e-b49a-151e50a0a099"
      },
      "outputs": [],
      "source": [
        "# Loading the dataset from the Excel file\n",
        "df = pd.read_excel('LabeledText.xlsx')\n",
        "\n",
        "# Keeping only the columns we need\n",
        "df = df[['Caption', 'LABEL']]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bccb45c-f827-41f0-a6cb-7cfa7d3d9b90",
      "metadata": {
        "id": "1bccb45c-f827-41f0-a6cb-7cfa7d3d9b90"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0efdbbf-dd0f-40f4-970d-aee2b6ffd6ef",
      "metadata": {
        "id": "b0efdbbf-dd0f-40f4-970d-aee2b6ffd6ef"
      },
      "outputs": [],
      "source": [
        "# Lowercasing the data in the 'Caption' column\n",
        "df['text_cleaned'] = df['Caption'].apply(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "802784f5-ab4c-452c-9a33-525713fd025c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "802784f5-ab4c-452c-9a33-525713fd025c",
        "outputId": "abb532d3-b54e-4dd3-bbcb-9cd582d843cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories\n",
            "['negative' 'positive' 'neutral']\n",
            "-------------\n",
            "Dataset Sample\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                 Caption  \\\n",
              "0                                                                          How I feel today #legday #jelly #aching #gym    \n",
              "1                    @ArrivaTW absolute disgrace two carriages from Bangor half way there standing room only #disgraced    \n",
              "2  This is my Valentine's from 1 of my nephews. I am elated; sometimes the little things are the biggest & best things!    \n",
              "3                          betterfeelingfilms: RT via Instagram: First day of filming #powerless back in 2011. Can't ¡­    \n",
              "4                                                                             Zoe's first love #Rattled @JohnnyHarper15    \n",
              "\n",
              "      LABEL  \\\n",
              "0  negative   \n",
              "1  negative   \n",
              "2  positive   \n",
              "3   neutral   \n",
              "4  positive   \n",
              "\n",
              "                                                                                                            text_cleaned  \n",
              "0                                                                          how i feel today #legday #jelly #aching #gym   \n",
              "1                    @arrivatw absolute disgrace two carriages from bangor half way there standing room only #disgraced   \n",
              "2  this is my valentine's from 1 of my nephews. i am elated; sometimes the little things are the biggest & best things!   \n",
              "3                          betterfeelingfilms: rt via instagram: first day of filming #powerless back in 2011. can't ¡­   \n",
              "4                                                                             zoe's first love #rattled @johnnyharper15   "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ed916ca-9438-4b5b-95f4-94d28875a3fa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Caption</th>\n",
              "      <th>LABEL</th>\n",
              "      <th>text_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How I feel today #legday #jelly #aching #gym</td>\n",
              "      <td>negative</td>\n",
              "      <td>how i feel today #legday #jelly #aching #gym</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@ArrivaTW absolute disgrace two carriages from Bangor half way there standing room only #disgraced</td>\n",
              "      <td>negative</td>\n",
              "      <td>@arrivatw absolute disgrace two carriages from bangor half way there standing room only #disgraced</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is my Valentine's from 1 of my nephews. I am elated; sometimes the little things are the biggest &amp; best things!</td>\n",
              "      <td>positive</td>\n",
              "      <td>this is my valentine's from 1 of my nephews. i am elated; sometimes the little things are the biggest &amp; best things!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>betterfeelingfilms: RT via Instagram: First day of filming #powerless back in 2011. Can't ¡­</td>\n",
              "      <td>neutral</td>\n",
              "      <td>betterfeelingfilms: rt via instagram: first day of filming #powerless back in 2011. can't ¡­</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Zoe's first love #Rattled @JohnnyHarper15</td>\n",
              "      <td>positive</td>\n",
              "      <td>zoe's first love #rattled @johnnyharper15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ed916ca-9438-4b5b-95f4-94d28875a3fa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ed916ca-9438-4b5b-95f4-94d28875a3fa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ed916ca-9438-4b5b-95f4-94d28875a3fa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b37c6f59-26f0-4d1b-b865-9ef7a777734d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b37c6f59-26f0-4d1b-b865-9ef7a777734d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b37c6f59-26f0-4d1b-b865-9ef7a777734d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4869,\n  \"fields\": [\n    {\n      \"column\": \"Caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4663,\n        \"samples\": [\n          \"This just turned my whole entire day around ??? ... One day she gone be mines \",\n          \"When your mirror foggy (me my self and lil sis ???????? \",\n          \"Absolutely incensed by this cynical 50 Shades/Valentine cash-in at Tesco \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LABEL\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"negative\",\n          \"positive\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4663,\n        \"samples\": [\n          \"this just turned my whole entire day around ??? ... one day she gone be mines \",\n          \"when your mirror foggy (me my self and lil sis ???????? \",\n          \"absolutely incensed by this cynical 50 shades/valentine cash-in at tesco \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# Printing unique categories/labels in the dataset\n",
        "print('Categories')\n",
        "print(df.LABEL.unique())\n",
        "print(\"-------------\")\n",
        "\n",
        "# Printing a sample of the dataset\n",
        "print('Dataset Sample')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fefc6705-a2a7-48f3-8d30-4f43215b3237",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fefc6705-a2a7-48f3-8d30-4f43215b3237",
        "outputId": "4907c711-f5a5-4111-8d95-54129f75d476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-e70d8c1119c7>:5: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  df['text_cleaned'] = df['text_cleaned'].apply(lambda x: BeautifulSoup(x, \"html.parser\").text)\n"
          ]
        }
      ],
      "source": [
        "# Removing URLs from the text\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: re.sub(r'http\\S+|www.\\S+', '', x))\n",
        "\n",
        "# Removing HTML tags from the text\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: BeautifulSoup(x, \"html.parser\").text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a062723e-3fbc-4778-a28f-6458bd68abd1",
      "metadata": {
        "id": "a062723e-3fbc-4778-a28f-6458bd68abd1"
      },
      "outputs": [],
      "source": [
        "# Defining a dictionary to convert common chat words to their full forms\n",
        "# There are many more chat words that can be added to this dictionary. These are some common examples.\n",
        "chat_words_dict = {\n",
        "    \"imo\": \"in my opinion\",\n",
        "    \"cyaa\": \"see you\",\n",
        "    \"idk\": \"I don't know\",\n",
        "    \"rn\": \"right now\",\n",
        "    \"afaik\": \"as far as I know\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da9bd3b-a57b-4880-9c1f-deca3d8eb6c1",
      "metadata": {
        "id": "3da9bd3b-a57b-4880-9c1f-deca3d8eb6c1"
      },
      "outputs": [],
      "source": [
        "# Converting chat words to their full forms in the 'text_cleaned' column\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(convert_chat_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8015b2f9-d9b4-408e-b888-46ced86dc481",
      "metadata": {
        "id": "8015b2f9-d9b4-408e-b888-46ced86dc481"
      },
      "outputs": [],
      "source": [
        "# Cleaning the text in the 'text_cleaned' column\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72e70bbc-e8bf-49bf-8a32-58d413a9df0f",
      "metadata": {
        "id": "72e70bbc-e8bf-49bf-8a32-58d413a9df0f"
      },
      "outputs": [],
      "source": [
        "# Removing special characters from the text in the 'text_cleaned' column\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(remove_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f5ddc0-7362-4f0c-8e0a-75f5f3eed126",
      "metadata": {
        "id": "36f5ddc0-7362-4f0c-8e0a-75f5f3eed126"
      },
      "outputs": [],
      "source": [
        "# Expanding contractions in the 'text_cleaned' column\n",
        "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: contractions.fix(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a74e0c-7fff-4bed-ac51-5fd114f931c7",
      "metadata": {
        "id": "41a74e0c-7fff-4bed-ac51-5fd114f931c7"
      },
      "outputs": [],
      "source": [
        "# Tokenizing the text in the 'text_cleaned' column\n",
        "df['tokens'] = df['text_cleaned'].apply(lambda x: word_tokenize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52dd4d0-aae6-452e-94be-e0d780f26319",
      "metadata": {
        "id": "d52dd4d0-aae6-452e-94be-e0d780f26319"
      },
      "outputs": [],
      "source": [
        "# Loading English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Removing stop words from the 'tokens' column\n",
        "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967c20af-3cb2-4e11-bc58-6c121c9dcb71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "967c20af-3cb2-4e11-bc58-6c121c9dcb71",
        "outputId": "ac746b3a-7027-4d01-bc29-163532dc0ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4849              [get, friday, night, look, sorted, newin, lbd, littleblackdress, strappy, plunge, neckline, mini, black, bodycon]\n",
            "4850                                                                                         [rt, nneagoe, love, caring, beautiful]\n",
            "4851               [february, winter, rainy, stormy, windy, wednesday, morning, love, happy, positive, passionate, reading, coffee]\n",
            "4852    [rt, thatguykai, honored, pittsburgh, pirates, consultant, coachotip, speak, ball, club, passionate, relentless, ownership]\n",
            "4853                                              [genghis, khan, ily, relatable, king, passionate, yeet, yas, sogengrn, apgenghis]\n",
            "4854                 [february, winter, rainy, stormy, windy, wednesday, evening, love, happy, positive, passionate, calm, fun, uk]\n",
            "4855                  [february, winter, rainy, stormy, windy, wednesday, evening, love, happy, positive, passionate, calm, coffee]\n",
            "4856                                                   [rt, bishopcarrollhs, great, bishopcarrollhs, students, caring, empowerbchs]\n",
            "4857                                         [big, thank, teachers, attended, isabcpdªso, nice, meet, passionate, educators, isabc]\n",
            "4858                                                                                                          [dave, looks, elated]\n",
            "4859                                                [dbel, scared, veryscared, holding, owt, hot, coming, nearer, david, petrified]\n",
            "4860                                           [completely, unique, petrified, palm, earrings, set, sterling, silver, fossil, gift]\n",
            "4861                                                               [rt, headquarters, fair, everything, makes, want, scream, anger]\n",
            "4862                                [fanghorn, forest, alder, woodland, glen, vorlich, lochearn, petrified, forest, lotr, scotland]\n",
            "4863                                                                                       [whisk, way, powerless, bakersgonnabake]\n",
            "4864                                                                   [omg, well, done, eskom, man, dies, loadshedding, powerless]\n",
            "4865                                                                                          [feelin, love, valentinesday, caring]\n",
            "4866                                                                                                           [blue, eyes, beaten]\n",
            "4867                                                                                      [la, chucha, louuu, te, chupo, los, ojos]\n",
            "4868                                      [colorsplashbw, zealous, remedios, herbales, tratamientos, naturales, remedios, herbales]\n",
            "Name: tokens, dtype: object\n",
            "\n",
            "\n",
            "                                                                                                                 Caption  \\\n",
            "0                                                                          How I feel today #legday #jelly #aching #gym    \n",
            "1                    @ArrivaTW absolute disgrace two carriages from Bangor half way there standing room only #disgraced    \n",
            "2  This is my Valentine's from 1 of my nephews. I am elated; sometimes the little things are the biggest & best things!    \n",
            "3                          betterfeelingfilms: RT via Instagram: First day of filming #powerless back in 2011. Can't ¡­    \n",
            "4                                                                             Zoe's first love #Rattled @JohnnyHarper15    \n",
            "\n",
            "      LABEL  \\\n",
            "0  negative   \n",
            "1  negative   \n",
            "2  positive   \n",
            "3   neutral   \n",
            "4  positive   \n",
            "\n",
            "                                                                                                   text_cleaned  \\\n",
            "0                                                                      how i feel today legday jelly aching gym   \n",
            "1              arrivatw absolute disgrace two carriages from bangor half way there standing room only disgraced   \n",
            "2  this is my valentines from of my nephews i am elated sometimes the little things are the biggest best things   \n",
            "3                            betterfeelingfilms rt via instagram first day of filming powerless back in cannot    \n",
            "4                                                                          zoes first love rattled johnnyharper   \n",
            "\n",
            "                                                                                         tokens  \n",
            "0                                                     [feel, today, legday, jelly, aching, gym]  \n",
            "1  [arrivatw, absolute, disgrace, two, carriages, bangor, half, way, standing, room, disgraced]  \n",
            "2               [valentines, nephews, elated, sometimes, little, things, biggest, best, things]  \n",
            "3                [betterfeelingfilms, rt, via, instagram, first, day, filming, powerless, back]  \n",
            "4                                                    [zoes, first, love, rattled, johnnyharper]  \n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4869 entries, 0 to 4868\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Caption       4869 non-null   object\n",
            " 1   LABEL         4869 non-null   object\n",
            " 2   text_cleaned  4869 non-null   object\n",
            " 3   tokens        4869 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 152.3+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Printing the updated 'tokens' column\n",
        "print(df['tokens'].tail(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Printing the first few rows of the DataFrame\n",
        "print(df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Printing the schema of the DataFrame\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a630482-316c-4b9e-a587-a6b7017eb32c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a630482-316c-4b9e-a587-a6b7017eb32c",
        "outputId": "b33be97d-1589-4575-c46c-4d59e5ef463e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4849             [get, friday, night, look, sort, newin, lbd, littleblackdress, strappy, plunge, neckline, mini, black, bodycon]\n",
            "4850                                                                                        [rt, nneagoe, love, care, beautiful]\n",
            "4851            [february, winter, rainy, stormy, windy, wednesday, morning, love, happy, positive, passionate, reading, coffee]\n",
            "4852    [rt, thatguykai, honor, pittsburgh, pirate, consultant, coachotip, speak, ball, club, passionate, relentless, ownership]\n",
            "4853                                           [genghis, khan, ily, relatable, king, passionate, yeet, yas, sogengrn, apgenghis]\n",
            "4854                 [february, winter, rainy, stormy, windy, wednesday, even, love, happy, positive, passionate, calm, fun, uk]\n",
            "4855                  [february, winter, rainy, stormy, windy, wednesday, even, love, happy, positive, passionate, calm, coffee]\n",
            "4856                                                   [rt, bishopcarrollhs, great, bishopcarrollhs, student, care, empowerbchs]\n",
            "4857                                          [big, thank, teacher, attend, isabcpdªso, nice, meet, passionate, educator, isabc]\n",
            "4858                                                                                                         [dave, look, elate]\n",
            "4859                                                      [dbel, scar, veryscared, hold, owt, hot, come, nearer, david, petrify]\n",
            "4860                                         [completely, unique, petrified, palm, earring, set, sterling, silver, fossil, gift]\n",
            "4861                                                             [rt, headquarters, fair, everything, make, want, scream, anger]\n",
            "4862                               [fanghorn, forest, alder, woodland, glen, vorlich, lochearn, petrify, forest, lotr, scotland]\n",
            "4863                                                                                    [whisk, way, powerless, bakersgonnabake]\n",
            "4864                                                                   [omg, well, do, eskom, man, die, loadshedding, powerless]\n",
            "4865                                                                                         [feelin, love, valentinesday, care]\n",
            "4866                                                                                                           [blue, eye, beat]\n",
            "4867                                                                                   [la, chucha, louuu, te, chupo, los, ojos]\n",
            "4868                                   [colorsplashbw, zealous, remedios, herbales, tratamientos, naturales, remedios, herbales]\n",
            "Name: tokens, dtype: object\n",
            "\n",
            "\n",
            "                                                                                                                 Caption  \\\n",
            "0                                                                          How I feel today #legday #jelly #aching #gym    \n",
            "1                    @ArrivaTW absolute disgrace two carriages from Bangor half way there standing room only #disgraced    \n",
            "2  This is my Valentine's from 1 of my nephews. I am elated; sometimes the little things are the biggest & best things!    \n",
            "3                          betterfeelingfilms: RT via Instagram: First day of filming #powerless back in 2011. Can't ¡­    \n",
            "4                                                                             Zoe's first love #Rattled @JohnnyHarper15    \n",
            "\n",
            "      LABEL  \\\n",
            "0  negative   \n",
            "1  negative   \n",
            "2  positive   \n",
            "3   neutral   \n",
            "4  positive   \n",
            "\n",
            "                                                                                                   text_cleaned  \\\n",
            "0                                                                      how i feel today legday jelly aching gym   \n",
            "1              arrivatw absolute disgrace two carriages from bangor half way there standing room only disgraced   \n",
            "2  this is my valentines from of my nephews i am elated sometimes the little things are the biggest best things   \n",
            "3                            betterfeelingfilms rt via instagram first day of filming powerless back in cannot    \n",
            "4                                                                          zoes first love rattled johnnyharper   \n",
            "\n",
            "                                                                                    tokens  \n",
            "0                                                  [feel, today, legday, jelly, ache, gym]  \n",
            "1  [arrivatw, absolute, disgrace, two, carriage, bangor, half, way, stand, room, disgrace]  \n",
            "2                   [valentine, nephew, elate, sometimes, little, thing, big, best, thing]  \n",
            "3              [betterfeelingfilms, rt, via, instagram, first, day, film, powerless, back]  \n",
            "4                                                [zoes, first, love, rattle, johnnyharper]  \n",
            "\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4869 entries, 0 to 4868\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   Caption       4869 non-null   object\n",
            " 1   LABEL         4869 non-null   object\n",
            " 2   text_cleaned  4869 non-null   object\n",
            " 3   tokens        4869 non-null   object\n",
            "dtypes: object(4)\n",
            "memory usage: 152.3+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# POS tag mapping dictionary\n",
        "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
        "\n",
        "# Create an instance of WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Applying lemmatization to the 'tokens' column\n",
        "df['tokens'] = df['tokens'].apply(lemmatize_text)\n",
        "\n",
        "# Printing the updated 'tokens' column after lemmatization\n",
        "print(df['tokens'].tail(20))\n",
        "print(\"\\n\")\n",
        "\n",
        "# Printing the first few rows of the DataFrame after lemmatization\n",
        "print(df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Printing the schema of the DataFrame after lemmatization\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f40b850-efbe-4b10-82ac-16a696c28156",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "8f40b850-efbe-4b10-82ac-16a696c28156",
        "outputId": "6364d9b7-8569-4a1d-98bb-c7c36ee060b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                                 Caption  \\\n",
              "0                                                                          How I feel today #legday #jelly #aching #gym    \n",
              "1                    @ArrivaTW absolute disgrace two carriages from Bangor half way there standing room only #disgraced    \n",
              "2  This is my Valentine's from 1 of my nephews. I am elated; sometimes the little things are the biggest & best things!    \n",
              "3                          betterfeelingfilms: RT via Instagram: First day of filming #powerless back in 2011. Can't ¡­    \n",
              "4                                                                             Zoe's first love #Rattled @JohnnyHarper15    \n",
              "\n",
              "      LABEL  \\\n",
              "0  negative   \n",
              "1  negative   \n",
              "2  positive   \n",
              "3   neutral   \n",
              "4  positive   \n",
              "\n",
              "                                                                                                   text_cleaned  \\\n",
              "0                                                                      how i feel today legday jelly aching gym   \n",
              "1              arrivatw absolute disgrace two carriages from bangor half way there standing room only disgraced   \n",
              "2  this is my valentines from of my nephews i am elated sometimes the little things are the biggest best things   \n",
              "3                            betterfeelingfilms rt via instagram first day of filming powerless back in cannot    \n",
              "4                                                                          zoes first love rattled johnnyharper   \n",
              "\n",
              "                                                                                    tokens  \n",
              "0                                                  [feel, today, legday, jelly, ache, gym]  \n",
              "1  [arrivatw, absolute, disgrace, two, carriage, bangor, half, way, stand, room, disgrace]  \n",
              "2                   [valentine, nephew, elate, sometimes, little, thing, big, best, thing]  \n",
              "3              [betterfeelingfilms, rt, via, instagram, first, day, film, powerless, back]  \n",
              "4                                                [zoes, first, love, rattle, johnnyharper]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1b34779-0837-48df-9251-4e31dc1ac2a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Caption</th>\n",
              "      <th>LABEL</th>\n",
              "      <th>text_cleaned</th>\n",
              "      <th>tokens</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How I feel today #legday #jelly #aching #gym</td>\n",
              "      <td>negative</td>\n",
              "      <td>how i feel today legday jelly aching gym</td>\n",
              "      <td>[feel, today, legday, jelly, ache, gym]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@ArrivaTW absolute disgrace two carriages from Bangor half way there standing room only #disgraced</td>\n",
              "      <td>negative</td>\n",
              "      <td>arrivatw absolute disgrace two carriages from bangor half way there standing room only disgraced</td>\n",
              "      <td>[arrivatw, absolute, disgrace, two, carriage, bangor, half, way, stand, room, disgrace]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is my Valentine's from 1 of my nephews. I am elated; sometimes the little things are the biggest &amp; best things!</td>\n",
              "      <td>positive</td>\n",
              "      <td>this is my valentines from of my nephews i am elated sometimes the little things are the biggest best things</td>\n",
              "      <td>[valentine, nephew, elate, sometimes, little, thing, big, best, thing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>betterfeelingfilms: RT via Instagram: First day of filming #powerless back in 2011. Can't ¡­</td>\n",
              "      <td>neutral</td>\n",
              "      <td>betterfeelingfilms rt via instagram first day of filming powerless back in cannot</td>\n",
              "      <td>[betterfeelingfilms, rt, via, instagram, first, day, film, powerless, back]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Zoe's first love #Rattled @JohnnyHarper15</td>\n",
              "      <td>positive</td>\n",
              "      <td>zoes first love rattled johnnyharper</td>\n",
              "      <td>[zoes, first, love, rattle, johnnyharper]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1b34779-0837-48df-9251-4e31dc1ac2a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e1b34779-0837-48df-9251-4e31dc1ac2a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e1b34779-0837-48df-9251-4e31dc1ac2a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5bcef277-4dd2-4ed0-85ff-a46e127558d3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5bcef277-4dd2-4ed0-85ff-a46e127558d3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5bcef277-4dd2-4ed0-85ff-a46e127558d3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4869,\n  \"fields\": [\n    {\n      \"column\": \"Caption\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4663,\n        \"samples\": [\n          \"This just turned my whole entire day around ??? ... One day she gone be mines \",\n          \"When your mirror foggy (me my self and lil sis ???????? \",\n          \"Absolutely incensed by this cynical 50 Shades/Valentine cash-in at Tesco \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LABEL\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"negative\",\n          \"positive\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_cleaned\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4532,\n        \"samples\": [\n          \"rt bruk russian footballs miss charming stripped of title after being exposed as a racist neonazi\",\n          \"do you like vampiresforget twilightthe addiction abelferrara movie about good evil and catharsis\",\n          \"bit of bleakness bleak blackandwhite columns trees\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf845611-93de-4094-b5f0-d5a54e83ecaa",
      "metadata": {
        "id": "bf845611-93de-4094-b5f0-d5a54e83ecaa"
      },
      "source": [
        "### Data splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cfeade-72d4-4f5e-b18f-52ad82365e38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3cfeade-72d4-4f5e-b18f-52ad82365e38",
        "outputId": "e47b736c-c39c-44f8-f79f-9daf135489a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 3408\n",
            "Validation set size: 730\n",
            "Test set size: 731\n"
          ]
        }
      ],
      "source": [
        "# Splitting the dataset into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['tokens'], df['LABEL'], test_size=0.3, random_state=12547392)\n",
        "\n",
        "# Further splitting the temporary set into validation and test sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12547392)\n",
        "\n",
        "# Printing the lengths of the training, validation, and test sets\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Validation set size: {len(X_val)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9deffdc2-0853-4e3b-87da-8688a5cb6e4d",
      "metadata": {
        "id": "9deffdc2-0853-4e3b-87da-8688a5cb6e4d"
      },
      "source": [
        "### Converting Text Data into BERT's Input Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f5fc36-d75b-4387-85d0-f33ee6fbb498",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f5fc36-d75b-4387-85d0-f33ee6fbb498",
        "outputId": "4ebbebc4-2f64-4629-f952-09e1bdc8b393"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label mapping:\n",
            "{'negative': 0, 'positive': 1, 'neutral': 2}\n"
          ]
        }
      ],
      "source": [
        "# Joining tokens back into full sentences\n",
        "df['text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Converting labels to numerical format if not already done\n",
        "label_mapping = {label: idx for idx, label in enumerate(df['LABEL'].unique())}\n",
        "df['label'] = df['LABEL'].map(label_mapping)\n",
        "\n",
        "# Printing unique labels and their mappings\n",
        "print(\"Label mapping:\")\n",
        "print(label_mapping)\n",
        "\n",
        "# Splitting the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['text'], df['label'], test_size=0.3, random_state=12547392)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12547392)\n",
        "\n",
        "# Initializing the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenizing the text data for training, validation, and test sets\n",
        "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, return_tensors='pt')\n",
        "val_encodings = tokenizer(list(X_val), truncation=True, padding=True, return_tensors='pt')\n",
        "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Converting labels to tensors for training, validation, and test sets\n",
        "train_labels_tensor = torch.tensor(y_train.tolist())\n",
        "val_labels_tensor = torch.tensor(y_val.tolist())\n",
        "test_labels_tensor = torch.tensor(y_test.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb61b01-78a8-4277-a3c7-7bd1f5754a24",
      "metadata": {
        "id": "dfb61b01-78a8-4277-a3c7-7bd1f5754a24"
      },
      "source": [
        "## BERT Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2cc0599-6ac9-4b6f-bf19-b3a1835a145c",
      "metadata": {
        "id": "f2cc0599-6ac9-4b6f-bf19-b3a1835a145c"
      },
      "source": [
        "### Custom dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b14c550-f150-462e-bdf9-c768e82f5f11",
      "metadata": {
        "id": "1b14c550-f150-462e-bdf9-c768e82f5f11"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        # Initializing the dataset with encodings and labels\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Properly handling tensor creation to avoid warnings\n",
        "        item = {key: torch.as_tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        # Adding label to the item\n",
        "        item['labels'] = torch.as_tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returning the total number of labels (dataset length)\n",
        "        return len(self.labels)\n",
        "\n",
        "# Creating the datasets with encodings and labels\n",
        "train_dataset = CustomDataset(train_encodings, train_labels_tensor)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels_tensor)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1f4820-e423-4805-971e-cdd64fcb746a",
      "metadata": {
        "id": "1f1f4820-e423-4805-971e-cdd64fcb746a"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49bc0b84-019d-42d9-b690-95305c03f9fc",
      "metadata": {
        "id": "49bc0b84-019d-42d9-b690-95305c03f9fc"
      },
      "outputs": [],
      "source": [
        "class MetricsCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.train_acc = []\n",
        "        self.val_acc = []\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if 'loss' in logs:\n",
        "            self.train_losses.append(logs['loss'])\n",
        "        if 'eval_loss' in logs:\n",
        "            self.val_losses.append(logs['eval_loss'])\n",
        "        if 'eval_accuracy' in logs:\n",
        "            self.val_acc.append(logs['eval_accuracy'])\n",
        "        if 'accuracy' in logs:\n",
        "            self.train_acc.append(logs['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "785336a3-7bcf-44b9-b5b3-dad22d7b0970",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "785336a3-7bcf-44b9-b5b3-dad22d7b0970",
        "outputId": "65a85fa4-e1e6-4f39-8845-98ba19297116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='264' max='639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [264/639 1:04:30 < 1:32:20, 0.07 it/s, Epoch 1.23/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.042100</td>\n",
              "      <td>1.030709</td>\n",
              "      <td>0.471233</td>\n",
              "      <td>0.410724</td>\n",
              "      <td>0.592851</td>\n",
              "      <td>0.471233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.735900</td>\n",
              "      <td>0.766671</td>\n",
              "      <td>0.657534</td>\n",
              "      <td>0.649501</td>\n",
              "      <td>0.667733</td>\n",
              "      <td>0.657534</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-d669eadac1fc>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Starting the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Evaluating the model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3248\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Loading the BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))\n",
        "\n",
        "# Training arguments with logging steps\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,                # log every 10 steps\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Initialize metrics callback\n",
        "metrics_callback = MetricsCallback()\n",
        "\n",
        "# Create the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # custom compute metrics function\n",
        "    callbacks=[metrics_callback]         # callback to save metrics\n",
        ")\n",
        "\n",
        "# Starting the training process\n",
        "trainer.train()\n",
        "\n",
        "# Evaluating the model on the validation set\n",
        "val_results = trainer.evaluate(eval_dataset=val_dataset)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# Creating a formatted report for validation and test results\n",
        "def print_formatted_results(results, title):\n",
        "    print(f\"\\n{title}:\")\n",
        "    for key, value in results.items():\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "\n",
        "# Printing formatted validation and test results\n",
        "print_formatted_results(val_results, \"Validation Results\")\n",
        "print_formatted_results(test_results, \"Test Results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c68de1-7bad-40f1-bc28-9036578a7611",
      "metadata": {
        "id": "25c68de1-7bad-40f1-bc28-9036578a7611"
      },
      "source": [
        "### Testing the model with BERT model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7e2009-d45b-436a-b564-e3e8270cdbb8",
      "metadata": {
        "id": "4d7e2009-d45b-436a-b564-e3e8270cdbb8"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model on the test set\n",
        "# Evaluating the model's performance on the test dataset to obtain various metrics\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# Printing test results in a formatted manner\n",
        "# Displaying the test results with improved formatting for better readability\n",
        "print(\"Test Results:\")\n",
        "print(f\"  eval_loss: {test_results['eval_loss']:.4f}\")  # Printing evaluation loss\n",
        "print(f\"  eval_accuracy: {test_results['eval_accuracy']:.4f}\")  # Printing evaluation accuracy\n",
        "print(f\"  eval_f1: {test_results['eval_f1']:.4f}\")  # Printing evaluation F1 score\n",
        "print(f\"  eval_precision: {test_results['eval_precision']:.4f}\")  # Printing evaluation precision\n",
        "print(f\"  eval_recall: {test_results['eval_recall']:.4f}\")  # Printing evaluation recall\n",
        "print(f\"  eval_runtime: {test_results['eval_runtime']:.4f}\")  # Printing evaluation runtime\n",
        "print(f\"  eval_samples_per_second: {test_results['eval_samples_per_second']:.4f}\")  # Printing evaluation samples per second\n",
        "print(f\"  eval_steps_per_second: {test_results['eval_steps_per_second']:.4f}\")  # Printing evaluation steps per second\n",
        "print(f\"  epoch: {test_results['epoch']:.4f}\")  # Printing the epoch number"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66c9dd0-efd1-47a3-99dc-0d2217804c77",
      "metadata": {
        "id": "e66c9dd0-efd1-47a3-99dc-0d2217804c77"
      },
      "source": [
        "### Plotting accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c95ced-14a2-4796-bdea-d82b2f2392f9",
      "metadata": {
        "id": "50c95ced-14a2-4796-bdea-d82b2f2392f9"
      },
      "outputs": [],
      "source": [
        "# Plotting the accuracies\n",
        "# Creating a figure with a specified size\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plotting training accuracy\n",
        "plt.plot(metrics_callback.train_acc, 'bo-', label='Training accuracy')\n",
        "\n",
        "# Plotting validation accuracy\n",
        "plt.plot(metrics_callback.val_acc, 'go-', label='Validation accuracy')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "# Adding a legend to the plot\n",
        "plt.legend()\n",
        "\n",
        "# Adding a grid to the plot\n",
        "plt.grid(True)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()\n",
        "\n",
        "# Plotting the losses\n",
        "# Creating a figure with a specified size\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plotting training loss\n",
        "plt.plot(metrics_callback.train_losses, 'bo-', label='Training loss')\n",
        "\n",
        "# Plotting validation loss\n",
        "plt.plot(metrics_callback.val_losses, 'go-', label='Validation loss')\n",
        "\n",
        "# Adding title and labels\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "\n",
        "# Adding a legend to the plot\n",
        "plt.legend()\n",
        "\n",
        "# Adding a grid to the plot\n",
        "plt.grid(True)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4612c72c-5fa8-411d-8b5c-e1a8e97100e9",
      "metadata": {
        "id": "4612c72c-5fa8-411d-8b5c-e1a8e97100e9"
      },
      "source": [
        "## RNN Classifier (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a991bbdc-6823-4eb0-ad8b-2b5b0e6319cd",
      "metadata": {
        "id": "a991bbdc-6823-4eb0-ad8b-2b5b0e6319cd"
      },
      "outputs": [],
      "source": [
        "# Assuming df is already loaded and preprocessed as done in the BERT implementation\n",
        "\n",
        "# Splitting the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['text'], df['label'], test_size=0.3, random_state=12547392)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12547392)\n",
        "\n",
        "# Preparing data for RNN model\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Converting data to numpy arrays\n",
        "train_data = np.array(X_train)\n",
        "val_data = np.array(X_val)\n",
        "test_data = np.array(X_test)\n",
        "\n",
        "# One-Hot Encoding labels\n",
        "y_train_1_hot = pd.get_dummies(y_train).values\n",
        "y_val_1_hot = pd.get_dummies(y_val).values\n",
        "y_test_1_hot = pd.get_dummies(y_test).values\n",
        "\n",
        "# Initializing and adapting the TextVectorization layer\n",
        "vectorizer = TextVectorization(max_tokens=100000, output_mode='int', ngrams=1, output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "vectorizer.adapt(train_data)\n",
        "\n",
        "# Initializing the embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((100000, EMBEDDING_DIM))\n",
        "\n",
        "# Building the RNN model with the provided optimal hyperparameters\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Input(shape=(1,), dtype=tf.string))  # Adding input layer\n",
        "model_rnn.add(vectorizer)  # Adding text vectorization layer\n",
        "model_rnn.add(Embedding(100000, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True))  # Adding embedding layer\n",
        "model_rnn.add(Dropout(0.2))  # Adding dropout layer\n",
        "model_rnn.add(Bidirectional(LSTM(200)))  # Adding bidirectional LSTM layer\n",
        "model_rnn.add(Dropout(0.2))  # Adding dropout layer\n",
        "model_rnn.add(Dense(50, activation='relu'))  # Adding dense layer with ReLU activation\n",
        "model_rnn.add(Dropout(0.2))  # Adding dropout layer\n",
        "model_rnn.add(Dense(len(y_train.unique()), activation='softmax'))  # Adding output layer with softmax activation\n",
        "\n",
        "# Compiling the model\n",
        "model_rnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
        "\n",
        "# Training the model with the optimal hyperparameters\n",
        "history = model_rnn.fit(train_data, y_train_1_hot, validation_data=(val_data, y_val_1_hot), batch_size=256, epochs=8, shuffle=True)\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "print(\"RNN Model Classification Report:\")\n",
        "predictions = model_rnn.predict(test_data)\n",
        "print(classification_report(y_test, np.argmax(predictions, axis=1)))\n",
        "\n",
        "# Plotting training history for accuracy\n",
        "plt.plot(history.history['categorical_accuracy'])\n",
        "plt.plot(history.history['val_categorical_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plotting training history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64be0ca7-1c6e-4174-b557-fa969ceeb260",
      "metadata": {
        "id": "64be0ca7-1c6e-4174-b557-fa969ceeb260"
      },
      "source": [
        "## CNN Classifier (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341367e3-495a-414d-be40-521917274155",
      "metadata": {
        "id": "341367e3-495a-414d-be40-521917274155"
      },
      "outputs": [],
      "source": [
        "# Assuming df is already loaded and preprocessed as done in the BERT implementation\n",
        "\n",
        "# Splitting the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df['text'], df['label'], test_size=0.3, random_state=12547392)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12547392)\n",
        "\n",
        "# One-Hot Encoding labels\n",
        "# Encoding labels for training, validation, and test sets\n",
        "y_train_1_hot = pd.get_dummies(y_train).values\n",
        "y_val_1_hot = pd.get_dummies(y_val).values\n",
        "y_test_1_hot = pd.get_dummies(y_test).values\n",
        "\n",
        "# Converting text (sequence of words) to sequence of indexes and padding the sequences\n",
        "MAX_WORDS = 100000\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "# Initializing and adapting the TextVectorization layer\n",
        "vectorizer = TextVectorization(max_tokens=MAX_WORDS, output_mode='int', output_sequence_length=MAX_SEQUENCE_LENGTH)\n",
        "vectorizer.adapt(X_train)\n",
        "\n",
        "# Initializing the embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
        "\n",
        "# Building CNN model with optimal hyperparameters\n",
        "FILTERS = 224\n",
        "DROPOUT_RATE = 0.3\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Creating model inputs\n",
        "inputs = Input(shape=(1,), dtype=tf.string)\n",
        "inputs_seq = vectorizer(inputs)\n",
        "\n",
        "# Creating embeddings and applying dropout\n",
        "embeddings = Embedding(MAX_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, trainable=True)(inputs_seq)\n",
        "dropped_embeddings = Dropout(rate=DROPOUT_RATE)(embeddings)\n",
        "\n",
        "# Creating multi-filter CNNs\n",
        "pooled_convs = []\n",
        "filter_sizes = [2, 3, 4]\n",
        "\n",
        "# Creating convolutional and pooling layers for each filter size\n",
        "for n_gram in filter_sizes:\n",
        "    conv = Conv1D(filters=FILTERS, kernel_size=n_gram, activation='relu')(dropped_embeddings)\n",
        "    pool = GlobalMaxPooling1D()(conv)\n",
        "    pooled_convs.append(pool)\n",
        "\n",
        "# Concatenating results from all filters and applying dropout\n",
        "concat = concatenate(pooled_convs)\n",
        "concat = Dropout(rate=DROPOUT_RATE)(concat)\n",
        "outputs = Dense(len(y_train.unique()), activation='softmax')(concat)\n",
        "\n",
        "# Compiling the model\n",
        "model_cnn = Model(inputs=inputs, outputs=outputs)\n",
        "model_cnn.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=LEARNING_RATE), metrics=[\"categorical_accuracy\"])\n",
        "\n",
        "# Converting numpy arrays to tensors for TensorFlow\n",
        "train_data = tf.convert_to_tensor(np.array(X_train), dtype=tf.string)\n",
        "val_data = tf.convert_to_tensor(np.array(X_val), dtype=tf.string)\n",
        "test_data = tf.convert_to_tensor(np.array(X_test), dtype=tf.string)\n",
        "\n",
        "# Initializing early stopping callback\n",
        "early_stopping = EarlyStopping(patience=10, verbose=2, monitor=\"val_categorical_accuracy\", mode=\"max\", restore_best_weights=True)\n",
        "\n",
        "# Training the model with the optimal hyperparameters\n",
        "history_cnn = model_cnn.fit(train_data, y_train_1_hot, validation_data=(val_data, y_val_1_hot), batch_size=128, epochs=20, shuffle=True, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "print(\"CNN Model Classification Report:\")\n",
        "predictions = np.argmax(model_cnn.predict(test_data), axis=1)\n",
        "print(classification_report(y_test, predictions))\n",
        "\n",
        "# Plotting training history for accuracy\n",
        "plt.plot(history_cnn.history['categorical_accuracy'])\n",
        "plt.plot(history_cnn.history['val_categorical_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plotting training history for loss\n",
        "plt.plot(history_cnn.history['loss'])\n",
        "plt.plot(history_cnn.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e0a8ac-3854-4da8-a4ba-c313f02e7a7f",
      "metadata": {
        "id": "a1e0a8ac-3854-4da8-a4ba-c313f02e7a7f"
      },
      "source": [
        "## BERT model hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b3e9ba9-c760-4a1c-b624-057f63a798f3",
      "metadata": {
        "id": "1b3e9ba9-c760-4a1c-b624-057f63a798f3"
      },
      "outputs": [],
      "source": [
        "# Creating the Optuna study for hyperparameter optimization\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Printing the best trial results and hyperparameters\n",
        "print(f\"Best trial: {study.best_trial.value}\")\n",
        "print(\"Best hyperparameters: \", study.best_trial.params)\n",
        "\n",
        "# Training the best model with the best hyperparameters\n",
        "# Extracting the best trial parameters\n",
        "best_trial = study.best_trial\n",
        "\n",
        "# Setting up training arguments with the best hyperparameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=best_trial.params['learning_rate'],\n",
        "    per_device_train_batch_size=best_trial.params['batch_size'],\n",
        "    per_device_eval_batch_size=best_trial.params['batch_size'],\n",
        "    num_train_epochs=best_trial.params['num_train_epochs'],\n",
        "    weight_decay=best_trial.params['weight_decay'],\n",
        "    warmup_steps=best_trial.params['warmup_steps'],\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    save_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10\n",
        ")\n",
        "\n",
        "# Initializing the Trainer with the best hyperparameters\n",
        "trainer = Trainer(\n",
        "    model_init=model_init,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Training the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluating the final model on the test set\n",
        "test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "\n",
        "# Printing the formatted test results\n",
        "print_formatted_results(test_results, \"Test Results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d99bc924-43b8-494a-98c8-9fda10ef7e4b",
      "metadata": {
        "id": "d99bc924-43b8-494a-98c8-9fda10ef7e4b"
      },
      "source": [
        "## Macro-averaged precision, recall, F1, and PR-AUC scores for each classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a53eb08-7c70-4ee0-8708-c67095571fa6",
      "metadata": {
        "id": "7a53eb08-7c70-4ee0-8708-c67095571fa6"
      },
      "outputs": [],
      "source": [
        "# Example usage for BERT\n",
        "# Generating predictions and probabilities for the BERT model on the test dataset\n",
        "y_pred_bert = trainer.predict(test_dataset).predictions.argmax(-1)\n",
        "y_proba_bert = trainer.predict(test_dataset).predictions\n",
        "\n",
        "# Evaluating the BERT model's performance using the custom evaluation function\n",
        "evaluate_classifier(y_test, y_pred_bert, y_proba_bert, classes=np.unique(y_test))\n",
        "\n",
        "# Example usage for RNN\n",
        "# Generating predictions for the RNN model on the test dataset\n",
        "predictions_rnn = model_rnn.predict(test_data)\n",
        "\n",
        "# Converting the predictions to label indices\n",
        "y_pred_rnn = np.argmax(predictions_rnn, axis=1)\n",
        "\n",
        "# Using the raw prediction probabilities for PR-AUC calculation\n",
        "y_proba_rnn = predictions_rnn\n",
        "\n",
        "# Evaluating the RNN model's performance using the custom evaluation function\n",
        "evaluate_classifier(y_test, y_pred_rnn, y_proba_rnn, classes=np.unique(y_test))\n",
        "\n",
        "# Example usage for CNN\n",
        "# Generating predictions for the CNN model on the test dataset\n",
        "predictions_cnn = model_cnn.predict(test_data)\n",
        "\n",
        "# Converting the predictions to label indices\n",
        "y_pred_cnn = np.argmax(predictions_cnn, axis=1)\n",
        "\n",
        "# Using the raw prediction probabilities for PR-AUC calculation\n",
        "y_proba_cnn = predictions_cnn\n",
        "\n",
        "# Evaluating the CNN model's performance using the custom evaluation function\n",
        "evaluate_classifier(y_test, y_pred_cnn, y_proba_cnn, classes=np.unique(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision-Recall AUC for each class and plotting the Precision-Recall curve"
      ],
      "metadata": {
        "id": "n5Fmwdo614bK"
      },
      "id": "n5Fmwdo614bK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating predictions and probabilities for the BERT model on the test dataset\n",
        "y_pred_bert = trainer.predict(test_dataset).predictions.argmax(-1)\n",
        "y_proba_bert = trainer.predict(test_dataset).predictions\n",
        "\n",
        "# Plotting Precision-Recall AUC for the BERT model\n",
        "plot_precision_recall_auc(y_test, y_proba_bert, [\"Negative\", \"Neutral\", \"Positive\"], \"BERT\")\n",
        "\n",
        "# Generating predictions for the RNN model on the test dataset\n",
        "predictions_rnn = model_rnn.predict(test_data)\n",
        "\n",
        "# Converting the predictions to label indices\n",
        "y_pred_rnn = np.argmax(predictions_rnn, axis=1)\n",
        "\n",
        "# Using the raw prediction probabilities for PR-AUC calculation\n",
        "y_proba_rnn = predictions_rnn\n",
        "\n",
        "# Plotting Precision-Recall AUC for the RNN model\n",
        "plot_precision_recall_auc(y_test, y_proba_rnn, [\"Negative\", \"Neutral\", \"Positive\"], \"RNN\")\n",
        "\n",
        "# Generating predictions for the CNN model on the test dataset\n",
        "predictions_cnn = model_cnn.predict(test_data)\n",
        "\n",
        "# Converting the predictions to label indices\n",
        "y_pred_cnn = np.argmax(predictions_cnn, axis=1)\n",
        "\n",
        "# Using the raw prediction probabilities for PR-AUC calculation\n",
        "y_proba_cnn = predictions_cnn\n",
        "\n",
        "# Plotting Precision-Recall AUC for the CNN model\n",
        "plot_precision_recall_auc(y_test, y_proba_cnn, [\"Negative\", \"Neutral\", \"Positive\"], \"CNN\")"
      ],
      "metadata": {
        "id": "jzhmNIMm1Fuh"
      },
      "id": "jzhmNIMm1Fuh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}